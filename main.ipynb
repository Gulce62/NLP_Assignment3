{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import requests\n",
    "import custom_lemmatizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gulce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\gulce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gulce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\gulce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\gulce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "with open('Fyodor Dostoyevski Processed.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Tokenize the text with the nltk library\n",
    "tokens = nltk.word_tokenize(text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['part', 'i', 'chapter', 'i', 'on', 'an', 'exceptionally', 'hot', 'evening', 'early', 'in', 'july', 'a', 'young', 'man', 'came', 'out', 'of', 'the', 'garret', 'in', 'which', 'he', 'lodged', 'in', 's.', 'place', 'and', 'walked', 'slowly', ',', 'as', 'though', 'in', 'hesitation', ',', 'towards', 'k.', 'bridge', '.', 'he', 'had', 'successfully', 'avoided', 'meeting', 'his', 'landlady', 'on', 'the', 'staircase', '.', 'his', 'garret', 'was', 'under', 'the', 'roof', 'of', 'a', 'high', ',', 'five-storied', 'house', 'and', 'was', 'more', 'like', 'a', 'cupboard', 'than', 'a', 'room', '.', 'the', 'landlady', 'who', 'provided', 'him', 'with', 'garret', ',', 'dinners', ',', 'and', 'attendance', ',', 'lived', 'on', 'the', 'floor', 'below', ',', 'and', 'every', 'time', 'he', 'went', 'out', 'he', 'was']\n"
     ]
    }
   ],
   "source": [
    "print(tokens[:100])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Find POS (part-of-speech) tags of the tokens with the nltk library\n",
    "pos_tags = nltk.pos_tag(tokens, tagset='universal')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('part', 'NOUN'), ('i', 'VERB'), ('chapter', 'NOUN'), ('i', 'NOUN'), ('on', 'ADP'), ('an', 'DET'), ('exceptionally', 'ADV'), ('hot', 'ADJ'), ('evening', 'VERB'), ('early', 'ADJ'), ('in', 'ADP'), ('july', 'NOUN'), ('a', 'DET'), ('young', 'ADJ'), ('man', 'NOUN'), ('came', 'VERB'), ('out', 'ADP'), ('of', 'ADP'), ('the', 'DET'), ('garret', 'NOUN'), ('in', 'ADP'), ('which', 'DET'), ('he', 'PRON'), ('lodged', 'VERB'), ('in', 'ADP'), ('s.', 'ADJ'), ('place', 'NOUN'), ('and', 'CONJ'), ('walked', 'VERB'), ('slowly', 'ADV'), (',', '.'), ('as', 'ADP'), ('though', 'ADP'), ('in', 'ADP'), ('hesitation', 'NOUN'), (',', '.'), ('towards', 'NOUN'), ('k.', 'VERB'), ('bridge', 'NOUN'), ('.', '.'), ('he', 'PRON'), ('had', 'VERB'), ('successfully', 'ADV'), ('avoided', 'VERB'), ('meeting', 'VERB'), ('his', 'PRON'), ('landlady', 'NOUN'), ('on', 'ADP'), ('the', 'DET'), ('staircase', 'NOUN'), ('.', '.'), ('his', 'PRON'), ('garret', 'NOUN'), ('was', 'VERB'), ('under', 'ADP'), ('the', 'DET'), ('roof', 'NOUN'), ('of', 'ADP'), ('a', 'DET'), ('high', 'ADJ'), (',', '.'), ('five-storied', 'ADJ'), ('house', 'NOUN'), ('and', 'CONJ'), ('was', 'VERB'), ('more', 'ADV'), ('like', 'ADP'), ('a', 'DET'), ('cupboard', 'NOUN'), ('than', 'ADP'), ('a', 'DET'), ('room', 'NOUN'), ('.', '.'), ('the', 'DET'), ('landlady', 'NOUN'), ('who', 'PRON'), ('provided', 'VERB'), ('him', 'PRON'), ('with', 'ADP'), ('garret', 'NOUN'), (',', '.'), ('dinners', 'NOUN'), (',', '.'), ('and', 'CONJ'), ('attendance', 'NOUN'), (',', '.'), ('lived', 'VERB'), ('on', 'ADP'), ('the', 'DET'), ('floor', 'NOUN'), ('below', 'ADP'), (',', '.'), ('and', 'CONJ'), ('every', 'DET'), ('time', 'NOUN'), ('he', 'PRON'), ('went', 'VERB'), ('out', 'ADP'), ('he', 'PRON'), ('was', 'VERB')]\n"
     ]
    }
   ],
   "source": [
    "print(pos_tags[:100])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Lemmatize the tokens using the WordNetLemmatizer in nltk with custom_lemmatizer class\n",
    "cm = custom_lemmatizer.custom_lemmatizer()\n",
    "lemmatized_tokens = [cm.lemmatize(pt) for pt in pos_tags]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['part', 'i', 'chapter', 'i', 'on', 'an', 'exceptionally', 'hot', 'evening', 'early', 'in', 'july', 'a', 'young', 'man', 'came', 'out', 'of', 'the', 'garret', 'in', 'which', 'he', 'lodged', 'in', 's.', 'place', 'and', 'walked', 'slowly', ',', 'as', 'though', 'in', 'hesitation', ',', 'towards', 'k.', 'bridge', '.', 'he', 'had', 'successfully', 'avoided', 'meeting', 'his', 'landlady', 'on', 'the', 'staircase', '.', 'his', 'garret', 'was', 'under', 'the', 'roof', 'of', 'a', 'high', ',', 'five-storied', 'house', 'and', 'was', 'more', 'like', 'a', 'cupboard', 'than', 'a', 'room', '.', 'the', 'landlady', 'who', 'provided', 'him', 'with', 'garret', ',', 'dinner', ',', 'and', 'attendance', ',', 'lived', 'on', 'the', 'floor', 'below', ',', 'and', 'every', 'time', 'he', 'went', 'out', 'he', 'was']\n"
     ]
    }
   ],
   "source": [
    "print(lemmatized_tokens[:100])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Get the stopwords list from https://gist.github.com/sebleier/554280\n",
    "response = requests.get(\"https://gist.githubusercontent.com/sebleier/554280/raw/7e0e4a1ce04c2bb7bd41089c9821dbcf6d0c786c/NLTK's%2520list%2520of%2520english%2520stopwords\")\n",
    "stopwords_list = response.text.split('\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', '']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "def find_collocation_candidates(tokens, pos_tags, window_size, min_freq=10):\n",
    "    # Create all bigrams according to the given window size\n",
    "    bigrams = []\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(1, window_size+1):\n",
    "            if i+j < len(tokens):\n",
    "                # Check whether the bigram is ADJ-NOUN or NOUN-NOUN form and ignore if it is not\n",
    "                word1, tag1 = pos_tags[i]\n",
    "                word2, tag2 = pos_tags[i+j]\n",
    "                if not((tag1 == 'ADJ' and tag2 == 'NOUN') or (tag1 == 'NOUN' and tag2 == 'NOUN')):\n",
    "                    continue\n",
    "                bigrams.append((tokens[i], tokens[i+j]))\n",
    "    bigrams_frequencies = {}\n",
    "    for b in bigrams:\n",
    "        if b in bigrams_frequencies:\n",
    "            bigrams_frequencies[b] += 1\n",
    "        else:\n",
    "            bigrams_frequencies[b] = 1\n",
    "    # Find the collocation candidates according to the required conditions\n",
    "    collocation_candidates = []\n",
    "    for b, f in bigrams_frequencies.items():\n",
    "        # Check whether the bigram occurs more than the desired minimum frequency and ignore if it is less\n",
    "        if f < min_freq:\n",
    "            continue\n",
    "        # Check whether the bigram contains punctuation marks and ignore if it contains\n",
    "        word1, word2 = b\n",
    "        if not (word1.isalpha() and word2.isalpha()):\n",
    "            continue\n",
    "        # Check whether the bigram contains stopwords and ignore if it contains\n",
    "        if word1 in stopwords_list or word2 in stopwords_list:\n",
    "            continue\n",
    "        # Add the bigram to collocation candidates list if it satisfies all the conditions\n",
    "        collocation_candidates.append(b)\n",
    "    return collocation_candidates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "collocation_candidates_1 = find_collocation_candidates(lemmatized_tokens, pos_tags, 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('young', 'man'), ('last', 'month'), ('dark', 'eye'), ('hay', 'market'), ('different', 'matter'), ('drunken', 'man'), ('unknown', 'reason'), ('stupid', 'thing'), ('old', 'woman'), ('good', 'thing')]\n"
     ]
    }
   ],
   "source": [
    "print(collocation_candidates_1[:10])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "collocation_candidates_3 = find_collocation_candidates(lemmatized_tokens, pos_tags, 3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('young', 'man'), ('matter', 'importance'), ('last', 'month'), ('dark', 'eye'), ('dark', 'hair'), ('time', 'time'), ('hay', 'market'), ('different', 'matter'), ('drunken', 'man'), ('unknown', 'reason')]\n"
     ]
    }
   ],
   "source": [
    "print(collocation_candidates_3[:10])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
