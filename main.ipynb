{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "import math\n",
    "import nltk\n",
    "import requests\n",
    "import custom_lemmatizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gulce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\gulce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gulce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\gulce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\gulce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "with open('Fyodor Dostoyevski Processed.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Tokenize the text with the nltk library\n",
    "tokens = nltk.word_tokenize(text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['part', 'i', 'chapter', 'i', 'on', 'an', 'exceptionally', 'hot', 'evening', 'early', 'in', 'july', 'a', 'young', 'man', 'came', 'out', 'of', 'the', 'garret', 'in', 'which', 'he', 'lodged', 'in', 's.', 'place', 'and', 'walked', 'slowly', ',', 'as', 'though', 'in', 'hesitation', ',', 'towards', 'k.', 'bridge', '.', 'he', 'had', 'successfully', 'avoided', 'meeting', 'his', 'landlady', 'on', 'the', 'staircase', '.', 'his', 'garret', 'was', 'under', 'the', 'roof', 'of', 'a', 'high', ',', 'five-storied', 'house', 'and', 'was', 'more', 'like', 'a', 'cupboard', 'than', 'a', 'room', '.', 'the', 'landlady', 'who', 'provided', 'him', 'with', 'garret', ',', 'dinners', ',', 'and', 'attendance', ',', 'lived', 'on', 'the', 'floor', 'below', ',', 'and', 'every', 'time', 'he', 'went', 'out', 'he', 'was']\n"
     ]
    }
   ],
   "source": [
    "print(tokens[:100])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Find POS (part-of-speech) tags of the tokens with the nltk library\n",
    "pos_tags = nltk.pos_tag(tokens, tagset='universal')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('part', 'NOUN'), ('i', 'VERB'), ('chapter', 'NOUN'), ('i', 'NOUN'), ('on', 'ADP'), ('an', 'DET'), ('exceptionally', 'ADV'), ('hot', 'ADJ'), ('evening', 'VERB'), ('early', 'ADJ'), ('in', 'ADP'), ('july', 'NOUN'), ('a', 'DET'), ('young', 'ADJ'), ('man', 'NOUN'), ('came', 'VERB'), ('out', 'ADP'), ('of', 'ADP'), ('the', 'DET'), ('garret', 'NOUN'), ('in', 'ADP'), ('which', 'DET'), ('he', 'PRON'), ('lodged', 'VERB'), ('in', 'ADP'), ('s.', 'ADJ'), ('place', 'NOUN'), ('and', 'CONJ'), ('walked', 'VERB'), ('slowly', 'ADV'), (',', '.'), ('as', 'ADP'), ('though', 'ADP'), ('in', 'ADP'), ('hesitation', 'NOUN'), (',', '.'), ('towards', 'NOUN'), ('k.', 'VERB'), ('bridge', 'NOUN'), ('.', '.'), ('he', 'PRON'), ('had', 'VERB'), ('successfully', 'ADV'), ('avoided', 'VERB'), ('meeting', 'VERB'), ('his', 'PRON'), ('landlady', 'NOUN'), ('on', 'ADP'), ('the', 'DET'), ('staircase', 'NOUN'), ('.', '.'), ('his', 'PRON'), ('garret', 'NOUN'), ('was', 'VERB'), ('under', 'ADP'), ('the', 'DET'), ('roof', 'NOUN'), ('of', 'ADP'), ('a', 'DET'), ('high', 'ADJ'), (',', '.'), ('five-storied', 'ADJ'), ('house', 'NOUN'), ('and', 'CONJ'), ('was', 'VERB'), ('more', 'ADV'), ('like', 'ADP'), ('a', 'DET'), ('cupboard', 'NOUN'), ('than', 'ADP'), ('a', 'DET'), ('room', 'NOUN'), ('.', '.'), ('the', 'DET'), ('landlady', 'NOUN'), ('who', 'PRON'), ('provided', 'VERB'), ('him', 'PRON'), ('with', 'ADP'), ('garret', 'NOUN'), (',', '.'), ('dinners', 'NOUN'), (',', '.'), ('and', 'CONJ'), ('attendance', 'NOUN'), (',', '.'), ('lived', 'VERB'), ('on', 'ADP'), ('the', 'DET'), ('floor', 'NOUN'), ('below', 'ADP'), (',', '.'), ('and', 'CONJ'), ('every', 'DET'), ('time', 'NOUN'), ('he', 'PRON'), ('went', 'VERB'), ('out', 'ADP'), ('he', 'PRON'), ('was', 'VERB')]\n"
     ]
    }
   ],
   "source": [
    "print(pos_tags[:100])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Lemmatize the tokens using the WordNetLemmatizer in nltk with custom_lemmatizer class\n",
    "cm = custom_lemmatizer.custom_lemmatizer()\n",
    "lemmatized_tokens = [cm.lemmatize(pt) for pt in pos_tags]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['part', 'i', 'chapter', 'i', 'on', 'an', 'exceptionally', 'hot', 'evening', 'early', 'in', 'july', 'a', 'young', 'man', 'came', 'out', 'of', 'the', 'garret', 'in', 'which', 'he', 'lodged', 'in', 's.', 'place', 'and', 'walked', 'slowly', ',', 'as', 'though', 'in', 'hesitation', ',', 'towards', 'k.', 'bridge', '.', 'he', 'had', 'successfully', 'avoided', 'meeting', 'his', 'landlady', 'on', 'the', 'staircase', '.', 'his', 'garret', 'was', 'under', 'the', 'roof', 'of', 'a', 'high', ',', 'five-storied', 'house', 'and', 'was', 'more', 'like', 'a', 'cupboard', 'than', 'a', 'room', '.', 'the', 'landlady', 'who', 'provided', 'him', 'with', 'garret', ',', 'dinner', ',', 'and', 'attendance', ',', 'lived', 'on', 'the', 'floor', 'below', ',', 'and', 'every', 'time', 'he', 'went', 'out', 'he', 'was']\n"
     ]
    }
   ],
   "source": [
    "print(lemmatized_tokens[:100])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Get the stopwords list from https://gist.github.com/sebleier/554280\n",
    "response = requests.get(\"https://gist.githubusercontent.com/sebleier/554280/raw/7e0e4a1ce04c2bb7bd41089c9821dbcf6d0c786c/NLTK's%2520list%2520of%2520english%2520stopwords\")\n",
    "stopwords_list = response.text.split('\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', '']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "def calculate_unigram_frequencies(tokens):\n",
    "    # Calculate the frequencies of all the unigrams\n",
    "    unigram_frequencies = {}\n",
    "    for t in tokens:\n",
    "        if t in unigram_frequencies:\n",
    "            unigram_frequencies[t] += 1\n",
    "        else:\n",
    "            unigram_frequencies[t] = 1\n",
    "    return unigram_frequencies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "def calculate_bigram_frequencies(tokens, window_size):\n",
    "    # Create all bigrams according to the given window size\n",
    "    bigrams = []\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(1, window_size+1):\n",
    "            if i+j < len(tokens):\n",
    "                bigrams.append((tokens[i], tokens[i+j]))\n",
    "    # Calculate the frequencies of all the bigrams\n",
    "    bigram_frequencies = {}\n",
    "    for b in bigrams:\n",
    "        if b in bigram_frequencies:\n",
    "            bigram_frequencies[b] += 1\n",
    "        else:\n",
    "            bigram_frequencies[b] = 1\n",
    "    all_bigrams = []\n",
    "    for b, f in bigram_frequencies.items():\n",
    "        all_bigrams.append(b)\n",
    "    return all_bigrams, bigram_frequencies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "def find_collocation_candidates(bigram_frequencies, pos_tags, min_freq):\n",
    "    # Find the collocation candidates according to the required conditions\n",
    "    collocation_candidates = []\n",
    "    for b, f in bigram_frequencies.items():\n",
    "        # Check whether the bigram occurs more than the desired minimum frequency and ignore if it is less\n",
    "        if f < min_freq:\n",
    "            continue\n",
    "        # Check whether the bigram contains punctuation marks and ignore if it contains\n",
    "        word1, word2 = b\n",
    "        if not (word1.isalpha() and word2.isalpha()):\n",
    "            continue\n",
    "        # Check whether the bigram contains stopwords and ignore if it contains\n",
    "        if word1 in stopwords_list or word2 in stopwords_list:\n",
    "            continue\n",
    "        # Check whether the bigram is ADJ-NOUN or NOUN-NOUN form and ignore if it is not\n",
    "        pos1 = [t for w, t in pos_tags if w == word1]\n",
    "        pos2 = [t for w, t in pos_tags if w == word2]\n",
    "        if not((\"ADJ\" in pos1 and \"NOUN\" in pos2) or (\"NOUN\" in pos1 and \"NOUN\" in pos2)):\n",
    "            continue\n",
    "        # Add the bigram to collocation candidates list if it satisfies all the conditions\n",
    "        collocation_candidates.append(b)\n",
    "    return collocation_candidates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [],
   "source": [
    "unigram_frequencies = calculate_unigram_frequencies(lemmatized_tokens)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "all_bigrams_1, bigram_frequencies_1 = calculate_bigram_frequencies(lemmatized_tokens, 1)\n",
    "collocation_candidates_1 = find_collocation_candidates(bigram_frequencies_1, pos_tags, 10)\n",
    "N_1 = sum(bigram_frequencies_1.values())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "all_bigrams_3, bigram_frequencies_3 = calculate_bigram_frequencies(lemmatized_tokens, 3)\n",
    "collocation_candidates_3 = find_collocation_candidates(bigram_frequencies_3, pos_tags, 10)\n",
    "N_3 = sum(bigram_frequencies_3.values())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "def calculate_t_score(c_bigram, c_word1, c_word2, N):\n",
    "    real_mean = c_bigram / N # p (MLE)\n",
    "    expected_mean = (c_word1 / N) * (c_word2 / N) # H0\n",
    "    variance = real_mean # for small p\n",
    "    t_score = (real_mean - expected_mean) / math.sqrt(variance / N)\n",
    "    return t_score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "def calculate_chi_square(c_bigram, c_word1, c_word2, N):\n",
    "    O11 = c_bigram # C(w1w2)\n",
    "    O12 = c_word1 - O11 # C(~w1w2)\n",
    "    O21 = c_word2 - O11 # C(w1~w2)\n",
    "    O22 = N - (O11 + O12 + O21) # C(~w1~w2)\n",
    "    chi_square = (N * (O11 * O22 - O12 * O21) ** 2) / ((O11 + O12) * (O11 + O21) * (O12 + O22) * (O21 + O22))\n",
    "    return chi_square"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "# TODO check the function\n",
    "def calculate_likelihood_ratio(c_bigram, c_w1, c_w2, N):\n",
    "    expected_c_bigram = c_w1 * c_w2 / N\n",
    "    if expected_c_bigram == 0:  # Prevent division by zero\n",
    "        return 0\n",
    "    return 2 * c_bigram * (math.log(c_bigram) - math.log(expected_c_bigram))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [],
   "source": [
    "# TODO make sorting algorithm outside of the function\n",
    "def calculate_scores(all_bigrams, bigram_frequencies, unigram_frequencies, N, sorting_score, window_size):\n",
    "    scores = []\n",
    "    for bigram in all_bigrams:\n",
    "        c_bigram = bigram_frequencies[bigram]\n",
    "        c_word1 = unigram_frequencies[bigram[0]] * window_size\n",
    "        c_word2 = unigram_frequencies[bigram[1]] * window_size\n",
    "        t_score = calculate_t_score(c_bigram, c_word1, c_word2, N)\n",
    "        chi_square = calculate_chi_square(c_bigram, c_word1, c_word2, N)\n",
    "        likelihood_ratio = calculate_likelihood_ratio(c_bigram, c_word1, c_word2, N)\n",
    "        scores.append((bigram, t_score, chi_square, likelihood_ratio, c_bigram, c_word1, c_word2))\n",
    "    # Sort by t-score (1) or chi-square (2) or likelihood ratio (3)\n",
    "    scores.sort(key=lambda x: x[sorting_score], reverse=True)\n",
    "    return scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO check the scores for 3 window size --> FAQ\n",
    "# TODO how to calculate N differs than word counts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [],
   "source": [
    "scores_1 = calculate_scores(collocation_candidates_1, bigram_frequencies_1, unigram_frequencies, N_1, 2, 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank\tBigram\t\t\tt-score\t\tc(w1w2)\tc(w1)\tc(w2)\n",
      "1\tstepan trofimovitch\t22.61907\t1387728.42608\t8094.58071\t512\t525\t513\n",
      "2\tippolit kirillovitch\t6.40293\t1359440.81390\t853.53909\t41\t43\t41\n",
      "3\tlef nicolaievitch\t6.40293\t1359440.81390\t853.53909\t41\t43\t41\n",
      "4\tavdotya romanovna\t10.58212\t1341882.35242\t2103.60415\t112\t119\t112\n",
      "5\tyulia mihailovna\t14.17530\t1326304.32483\t3535.43428\t201\t215\t202\n",
      "6\tnikodim fomitch\t4.89889\t1316081.53843\t523.78161\t24\t24\t26\n",
      "7\tlizabetha prokofievna\t13.07494\t1273169.85230\t3049.05704\t171\t185\t177\n",
      "8\tmavriky nikolaevitch\t11.48792\t1263071.67646\t2419.89453\t132\t149\t132\n",
      "9\ttrifon borissovitch\t6.24480\t1235650.86652\t808.35698\t39\t45\t39\n",
      "10\trodion romanovitch\t9.05477\t1205266.43226\t1573.66241\t82\t97\t82\n",
      "11\tmihail makarovitch\t4.58250\t1197632.51995\t459.95618\t21\t25\t21\n",
      "12\tgavrila ardalionovitch\t7.61540\t1173526.55889\t1150.15044\t58\t61\t67\n",
      "13\tarina prohorovna\t6.24478\t1120123.57257\t800.70088\t39\t44\t44\n",
      "14\tvarvara petrovna\t20.53443\t1056418.46526\t6604.72238\t422\t474\t507\n",
      "15\tsemyon yakovlevitch\t6.08254\t1034362.56836\t757.64070\t37\t51\t37\n",
      "16\tkuzma kuzmitch\t4.47204\t983274.48267\t432.11671\t20\t29\t20\n",
      "17\tdaria alexeyevna\t4.35881\t980371.30086\t412.34766\t19\t21\t25\n",
      "18\tdarya pavlovna\t6.99963\t935804.97306\t966.02147\t49\t62\t59\n",
      "19\tkaterina ivanovna\t20.23906\t883755.63612\t6294.28852\t410\t427\t635\n",
      "20\tpyotr stepanovitch\t22.54783\t869957.82844\t7577.97170\t509\t834\t509\n"
     ]
    }
   ],
   "source": [
    "print(\"Rank\\tBigram\\t\\t\\tt-score\\t\\tc(w1w2)\\tc(w1)\\tc(w2)\")\n",
    "for i, (bigram, t_score, chi_square, likelihood_ratio, c_bigram, c_w1, c_w2) in enumerate(scores_1[:20]):\n",
    "    print(f\"{i+1}\\t{bigram[0]} {bigram[1]}\\t{t_score:.5f}\\t{chi_square:.5f}\\t{likelihood_ratio:.5f}\\t{c_bigram}\\t{c_w1}\\t{c_w2}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [],
   "source": [
    "scores_3 = calculate_scores(collocation_candidates_3, bigram_frequencies_3, unigram_frequencies, N_3, 2, 3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank\tBigram\t\t\tt-score\t\tc(w1w2)\tc(w1)\tc(w2)\n",
      "1\tstepan trofimovitch\t22.60237\t461893.15774\t6969.60101\t512\t1575\t1539\n",
      "2\tippolit kirillovitch\t6.40254\t453091.95355\t763.45283\t41\t129\t123\n",
      "3\tlef nicolaievitch\t6.40254\t453091.95355\t763.45283\t41\t129\t123\n",
      "4\tavdotya romanovna\t10.58036\t447144.47115\t1857.51484\t112\t357\t336\n",
      "5\tyulia mihailovna\t14.17100\t441833.13438\t3093.79186\t201\t645\t606\n",
      "6\tnikodim fomitch\t4.89871\t438661.53850\t471.04819\t24\t72\t78\n",
      "7\tlizabetha prokofievna\t13.07143\t424161.65646\t2673.33140\t171\t555\t531\n",
      "8\tmavriky nikolaevitch\t11.48552\t420847.59895\t2129.86070\t132\t447\t396\n",
      "9\ttrifon borissovitch\t6.24441\t411831.33350\t722.66516\t39\t135\t117\n",
      "10\trodion romanovitch\t9.05354\t401645.86345\t1393.48988\t82\t291\t246\n",
      "11\tmihail makarovitch\t4.58233\t399182.56006\t413.81444\t21\t75\t63\n",
      "12\tgavrila ardalionovitch\t7.61464\t391097.91263\t1022.71133\t58\t183\t201\n",
      "13\tarina prohorovna\t6.24435\t373322.26272\t715.00906\t39\t132\t132\n",
      "14\tvarvara petrovna\t20.51802\t351576.63718\t5677.49301\t422\t1422\t1521\n",
      "15\tsemyon yakovlevitch\t6.08211\t344737.94811\t676.34334\t37\t153\t111\n",
      "16\to clock\t13.22185\t334914.47981\t2645.15855\t175\t675\t579\n",
      "17\tkuzma kuzmitch\t4.47186\t327731.26451\t388.17219\t20\t87\t60\n",
      "18\tdaria alexeyevna\t4.35865\t326764.87125\t370.60037\t19\t63\t75\n",
      "19\twisp tow\t3.74150\t323415.10193\t281.33552\t14\t54\t48\n",
      "20\tdarya pavlovna\t6.99890\t311869.44019\t858.35740\t49\t186\t177\n"
     ]
    }
   ],
   "source": [
    "print(\"Rank\\tBigram\\t\\t\\tt-score\\t\\tc(w1w2)\\tc(w1)\\tc(w2)\")\n",
    "for i, (bigram, t_score, chi_square, likelihood_ratio, c_bigram, c_w1, c_w2) in enumerate(scores_3[:20]):\n",
    "    print(f\"{i+1}\\t{bigram[0]} {bigram[1]}\\t{t_score:.5f}\\t{chi_square:.5f}\\t{likelihood_ratio:.5f}\\t{c_bigram}\\t{c_w1}\\t{c_w2}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
